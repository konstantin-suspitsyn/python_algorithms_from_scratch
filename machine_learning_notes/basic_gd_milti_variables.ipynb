{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_gd_milti_variables.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nq6T_wkPzOh"
      },
      "source": [
        "# Градиентный спуск для одной переменной и свободного члена"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdpMfFvwPvrm"
      },
      "source": [
        "## Вступление<br>\n",
        "Это второй ноутбук в серии, где я буду с нуля создавать базовые алгоритмы машинного обучения с объяснениями и дополнительными материлами.<br>\n",
        "Так я сам разберусь в мелочах, а впоследствии, может помогу еще кому-нибудь<br>В ноутбуке [basic_gradient_descent.ipynb](https://github.com/konstantin-suspitsyn/algorithms-from-scratch/blob/main/basic_gradient_descent.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwFG84T1Ro6X"
      },
      "source": [
        "## Теоретическая часть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMayrp29Rupg"
      },
      "source": [
        "Теория аналогична той, что была в файле basic_gradient_descent.ipynb\n",
        "<br><br>\n",
        "$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 $<br>\n",
        "Частные производные по $a_i$ равны $ \\frac{\\delta f}{\\delta a} = \\frac{1}{n} \\sum_{i=1}^n-2*(y-(a_i^0*x_i^0+a_i^1*x_i^1+...+a_i^z*x_i^z))*x_i $, где $x^0=1$, а $a^0$ — свободный член, а сверху не степень, а порчдковый номер атрибута <br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp-AF5HBp_hE"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM9e0GyhRsAR"
      },
      "source": [
        "class GradientDescents:\n",
        "  \n",
        "  '''\n",
        "  Имплементация градиентного спуска с нуля\n",
        "  '''\n",
        "\n",
        "  def progress_tracker(self, step: int, cost_function: float) -> None:\n",
        "    '''\n",
        "    Функция позволяет отслеживать онлайн прогресс\n",
        "\n",
        "    :param step: текущий шаг\n",
        "    :param cost_function: значение кост-функции в данный момент\n",
        "\n",
        "    '''\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "    print('Шаг: {}'.format(step))\n",
        "    print('Функция потерь: {:.2f}'.format(cost_function))\n",
        "\n",
        "  def mse_function(self, y_true: list, y_pred: list) -> float:\n",
        "    '''\n",
        "    Функция, которая считет MSE\n",
        "\n",
        "    :param y_true: значения y, которые мы знаем из фактических данных\n",
        "    :param y_pred: значения y, которые мы получили в данный момент\n",
        "\n",
        "    :return mse: значеник MSE по формуле\n",
        "    '''\n",
        "    # Кол-во значений, которое мы сравнивам\n",
        "    n = len(y_true)\n",
        "    # Стартуем с нуля\n",
        "    pre_mse = 0\n",
        "    for index, value in enumerate(y_true):\n",
        "      pre_mse += (value - y_pred[index])**2\n",
        "    mse = pre_mse/n\n",
        "    return mse\n",
        "  \n",
        "  def gradient_descent(self, X_true: list, y_true: list, \\\n",
        "                       start_a: float = 1.0, start_b: float = 1.0, \\\n",
        "                       learning_rate: float = 0.003, max_steps: int =30000, \\\n",
        "                       save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Простейший градиентный спуск для формул вида y=a*x+b\n",
        "\n",
        "    :param start_a: стартовое значение a\n",
        "    :param start_b: стартовое значение b\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                       если значение отличное от нуля, \n",
        "                       будет сохраняться каждый i-ый шаг\n",
        "\n",
        "    :return a: значение a\n",
        "    :return b: значение b\n",
        "    :return mse: значение MSE по формуле\n",
        "    :return mse_list: значение списка MSE по формуле для графика\n",
        "    :return a_list: значение a для графика\n",
        "    :return b_list: значение b для графика\n",
        "    '''\n",
        "    # Инициализируем первый шаг\n",
        "    step = 0\n",
        "    a = start_a\n",
        "    b = start_b\n",
        "    mse = 9999999\n",
        "    mse_prev = 0\n",
        "\n",
        "    # Сделаем трекинг обучения\n",
        "    mse_list = []\n",
        "    a_list = []\n",
        "    b_list = []\n",
        "\n",
        "    # Список посчитанных y для кост-функции\n",
        "    y_pred = []\n",
        "    # количество элементов в предсказаниях/реальных данных\n",
        "    n=len(y_true)\n",
        "\n",
        "    # Создаем переменные градиентов\n",
        "    grad_a=0\n",
        "    grad_b=0\n",
        "\n",
        "    # модель будет работать пока мы не пройдем все шаги, \n",
        "    # или разница по MSE не будет меньше 1e-10\n",
        "    # или разница между MSE и MSE_prev меньше 1e-10\n",
        "    while (step <= max_steps) and (mse >= 1e-10) \\\n",
        "           and (abs(mse - mse_prev) >= 1e-5):\n",
        "      \n",
        "      mse_prev = mse\n",
        "      # Вычисляем сдвиги, как было описано в теории\n",
        "      for i, x in enumerate(X_true):\n",
        "        grad_a += -2*(y_true[i] - (a*x + b))* x\n",
        "        grad_b += -2*(y_true[i] - (a*x + b))\n",
        "      grad_a = grad_a/n\n",
        "      grad_b = grad_b/n\n",
        "      # Делаем сдвиг в сторону противоположную градиенту с учетом lr\n",
        "      a -= learning_rate*grad_a\n",
        "      b -= learning_rate*grad_b\n",
        "      # Считаем новые предасказания\n",
        "      y_pred = [a*x+b for x in X_true]\n",
        "      # Проверяем MSE\n",
        "      mse = self.mse_function(y_true, y_pred)\n",
        "\n",
        "      step += 1\n",
        "\n",
        "      # Заполняем данные для отслеживания прогресса\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          a_list.append(a)\n",
        "          b_list.append(b)\n",
        "      \n",
        "      self.progress_tracker(step-1, mse)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1, \\\n",
        "            'mse_list': mse_list, 'a_list': a_list, 'b_list': b_list}\n",
        "    else:\n",
        "      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjwrH-UX9cWr"
      },
      "source": [
        "Создаем синтетические данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BJU6KgGdohN"
      },
      "source": [
        "# Веса\n",
        "s_w = [5, 12, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH_MZAv5dopv"
      },
      "source": [
        "X_true = []\n",
        "\n",
        "for i in range(100):\n",
        "  x_element = []\n",
        "  for j in range(3):\n",
        "    x_element.append(random.random())\n",
        "  X_true.append(x_element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNqUkUkdosq"
      },
      "source": [
        "y_true = []\n",
        "for xt in X_true:\n",
        "  y_true.append(sum([w*x for w,x in zip(s_w,xt)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYUKb4EW-vex"
      },
      "source": [
        "old_gb = GradientDescents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJc3wXR68YnJ"
      },
      "source": [
        "# Создаем рандомный спиок весов данных равный количеству атрибутов\n",
        "weights = [random.random() for f in X_true[0]]\n",
        "\n",
        "learning_rate = 0.001\n",
        "mse=999\n",
        "\n",
        "n = len(X_true)\n",
        "st = 0\n",
        "while mse>1e-5:\n",
        "  # Считаем градиенты\n",
        "  gradients = []\n",
        "  for wi, w_value in enumerate(weights):\n",
        "    current_gradient=0\n",
        "    for yi, y_t_val in enumerate(y_true):\n",
        "      current_gradient += -2*(y_t_val - sum([w*x for w,x in zip(weights,X_true[yi])]))* X_true[yi][wi]\n",
        "    current_gradient = current_gradient/n\n",
        "    gradients.append(current_gradient)\n",
        "\n",
        "  # Делаем сдвиг весов\n",
        "  for gi, gr_value in enumerate(gradients):\n",
        "    weights[gi] = weights[gi] - learning_rate*gr_value\n",
        "\n",
        "  #Считаем y_pred\n",
        "  y_pred = []\n",
        "  for X_current in X_true:\n",
        "    y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n",
        "  st +=1\n",
        "  mse = old_gb.mse_function(y_true, y_pred)\n",
        "  old_gb.progress_tracker(st, mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhdeGNfXb4R-",
        "outputId": "f9e5c206-6323-4503-e1ba-c4254924202d"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.000588636309069, 11.997679793629745, 4.001396190588351]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LUaReSQ8yIN",
        "outputId": "269c6842-56dd-4715-8044-b34d1906b189"
      },
      "source": [
        "s_w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 12, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePAfrto5hZL_"
      },
      "source": [
        "Алгоритм отработал. Веса почти сошлись"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT6idvg2ha7V"
      },
      "source": [
        "Доработка класса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXWCBX--hguc"
      },
      "source": [
        "class GradientDescents:\n",
        "  \n",
        "  '''\n",
        "  Имплементация градиентного спуска с нуля\n",
        "  '''\n",
        "\n",
        "  import random\n",
        "\n",
        "  def progress_tracker(self, step: int, cost_function: float) -> None:\n",
        "    '''\n",
        "    Функция позволяет отслеживать онлайн прогресс\n",
        "\n",
        "    :param step: текущий шаг\n",
        "    :param cost_function: значение кост-функции в данный момент\n",
        "\n",
        "    '''\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "    print('Шаг: {}'.format(step))\n",
        "    print('Функция потерь: {:.2f}'.format(cost_function))\n",
        "\n",
        "  def mse_function(self, y_true: list, y_pred: list) -> float:\n",
        "    '''\n",
        "    Функция, которая считет MSE\n",
        "\n",
        "    :param y_true: значения y, которые мы знаем из фактических данных\n",
        "    :param y_pred: значения y, которые мы получили в данный момент\n",
        "\n",
        "    :return mse: значеник MSE по формуле\n",
        "    '''\n",
        "    # Кол-во значений, которое мы сравнивам\n",
        "    n = len(y_true)\n",
        "    # Стартуем с нуля\n",
        "    pre_mse = 0\n",
        "    for index, value in enumerate(y_true):\n",
        "      pre_mse += (value - y_pred[index])**2\n",
        "    mse = pre_mse/n\n",
        "    return mse\n",
        "  \n",
        "  def gradient_descent(self, X_true: list, y_true: list, \\\n",
        "                       start_a: float = 1.0, start_b: float = 1.0, \\\n",
        "                       learning_rate: float = 0.003, max_steps: int =30000, \\\n",
        "                       save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Простейший градиентный спуск для формул вида y=a*x+b\n",
        "\n",
        "    :param start_a: стартовое значение a\n",
        "    :param start_b: стартовое значение b\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                       если значение отличное от нуля, \n",
        "                       будет сохраняться каждый i-ый шаг\n",
        "\n",
        "    :return return_dict: {\n",
        "                :return a: значение a\n",
        "                :return b: значение b\n",
        "                :return mse: значение MSE по формуле\n",
        "                :return steps: кол-во прошедших шагов\n",
        "                :return mse_list: значение списка MSE по формуле для графика, если save_steps > 0\n",
        "                :return a_list: значение a для графика, если save_steps > 0\n",
        "                :return b_list: значение b для графика, если save_steps > 0\n",
        "                        }\n",
        "    '''\n",
        "    # Инициализируем первый шаг\n",
        "    step = 0\n",
        "    a = start_a\n",
        "    b = start_b\n",
        "    mse = 9999999\n",
        "    mse_prev = 0\n",
        "\n",
        "    # Сделаем трекинг обучения\n",
        "    mse_list = []\n",
        "    a_list = []\n",
        "    b_list = []\n",
        "\n",
        "    # Список посчитанных y для кост-функции\n",
        "    y_pred = []\n",
        "    # количество элементов в предсказаниях/реальных данных\n",
        "    n=len(y_true)\n",
        "\n",
        "    # Создаем переменные градиентов\n",
        "    grad_a=0\n",
        "    grad_b=0\n",
        "\n",
        "    # модель будет работать пока мы не пройдем все шаги, \n",
        "    # или разница по MSE не будет меньше 1e-10\n",
        "    # или разница между MSE и MSE_prev меньше 1e-10\n",
        "    while (step <= max_steps) and (mse >= 1e-10) \\\n",
        "           and (abs(mse - mse_prev) >= 1e-5):\n",
        "      \n",
        "      mse_prev = mse\n",
        "      # Вычисляем сдвиги, как было описано в теории\n",
        "      for i, x in enumerate(X_true):\n",
        "        grad_a += -2*(y_true[i] - (a*x + b))* x\n",
        "        grad_b += -2*(y_true[i] - (a*x + b))\n",
        "      grad_a = grad_a/n\n",
        "      grad_b = grad_b/n\n",
        "      # Делаем сдвиг в сторону противоположную градиенту с учетом lr\n",
        "      a -= learning_rate*grad_a\n",
        "      b -= learning_rate*grad_b\n",
        "      # Считаем новые предасказания\n",
        "      y_pred = [a*x+b for x in X_true]\n",
        "      # Проверяем MSE\n",
        "      mse = self.mse_function(y_true, y_pred)\n",
        "\n",
        "      step += 1\n",
        "\n",
        "      # Заполняем данные для отслеживания прогресса\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          a_list.append(a)\n",
        "          b_list.append(b)\n",
        "      \n",
        "      self.progress_tracker(step-1, mse)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1, \\\n",
        "            'mse_list': mse_list, 'a_list': a_list, 'b_list': b_list}\n",
        "    else:\n",
        "      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict\n",
        "\n",
        "  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n",
        "                              weights: list = None, max_steps: int = 10000, \\\n",
        "                              learning_rate: float = 0.003, \\\n",
        "                              save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Градиентный спуск для нескольких переменных\n",
        "\n",
        "    :param X_true: фактические аттрибуты\n",
        "    :param y_true: фактические результаты\n",
        "    :param weights: стартовые веса, если не хотим начать обучение с рандома\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                        если значение отличное от нуля, \n",
        "                        будет сохраняться каждый i-ый шаг\n",
        "    \n",
        "    '''\n",
        "    # Создаем рандомный спиок весов данных равный количеству атрибутов\n",
        "    if weights == None:\n",
        "      weights = [self.random.random() for f in X_true[0]]\n",
        "\n",
        "    if save_steps > 0:\n",
        "      mse_list = []\n",
        "      weights_list = []\n",
        "    \n",
        "    # MSE предыдущего шага\n",
        "    mse_prev = 0\n",
        "    mse = 999\n",
        "\n",
        "    # Кол-во экспериментов, которые у нас есть      \n",
        "    n = len(X_true)\n",
        "\n",
        "    step = 0\n",
        "    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n",
        "      # Считаем градиенты\n",
        "      gradients = []\n",
        "      for wi, w_value in enumerate(weights):\n",
        "        current_gradient=0\n",
        "        for yi, y_t_val in enumerate(y_true):\n",
        "          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n",
        "                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n",
        "        current_gradient = current_gradient/n\n",
        "        gradients.append(current_gradient)\n",
        "\n",
        "      # Делаем сдвиг весов\n",
        "      for gi, gr_value in enumerate(gradients):\n",
        "        weights[gi] = weights[gi] - learning_rate*gr_value\n",
        "\n",
        "      #Считаем y_pred\n",
        "      y_pred = []\n",
        "      for X_current in X_true:\n",
        "        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n",
        "      \n",
        "      step +=1\n",
        "      mse_prev = mse\n",
        "      mse = self.mse_function(y_true, y_pred)\n",
        "      self.progress_tracker(step, mse)\n",
        "\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          weights_list.append(weights)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n",
        "                      'mse_list': mse_list, 'weights_list': weights_list}\n",
        "    else:\n",
        "      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msvMU6d8wL1Z"
      },
      "source": [
        "Опробуем что получилось на Boston Dataset из sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OTITmzT5QcI"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "X, y = load_boston(return_X_y=True)\n",
        "X_true = []\n",
        "for i in X:\n",
        "  x_s_list = [f for f in i]\n",
        "  # x_s_list.append(1)\n",
        "  X_true.append(x_s_list)\n",
        "y_true = [f for f in y]\n",
        "del X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KMhNoANb4VA"
      },
      "source": [
        "grad_count = GradientDescents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuAL9L5X-ouA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3329c2-a025-4d15-9e38-3e89c7c3a7f9"
      },
      "source": [
        "grad = grad_count.gradient_descent_multi(X_true, y_true, learning_rate=0.000003, max_steps=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Шаг: 10001\n",
            "Функция потерь: 49.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNtp1-PvnZEt"
      },
      "source": [
        "На самом деле, функция gradient_descent является частным вариантом функции gradient_descent_multi, поэтому добавим ее внутрь gradient_descent_multi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrECU8_R3u-f"
      },
      "source": [
        "class GradientDescents:\n",
        "  \n",
        "  '''\n",
        "  Имплементация градиентного спуска с нуля\n",
        "  '''\n",
        "\n",
        "  import random\n",
        "\n",
        "  def progress_tracker(self, step: int, cost_function: float) -> None:\n",
        "    '''\n",
        "    Функция позволяет отслеживать онлайн прогресс\n",
        "\n",
        "    :param step: текущий шаг\n",
        "    :param cost_function: значение кост-функции в данный момент\n",
        "\n",
        "    '''\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "    print('Шаг: {}'.format(step))\n",
        "    print('Функция потерь: {:.2f}'.format(cost_function))\n",
        "\n",
        "  def mse_function(self, y_true: list, y_pred: list) -> float:\n",
        "    '''\n",
        "    Функция, которая считет MSE\n",
        "\n",
        "    :param y_true: значения y, которые мы знаем из фактических данных\n",
        "    :param y_pred: значения y, которые мы получили в данный момент\n",
        "\n",
        "    :return mse: значеник MSE по формуле\n",
        "    '''\n",
        "    # Кол-во значений, которое мы сравнивам\n",
        "    n = len(y_true)\n",
        "    # Стартуем с нуля\n",
        "    pre_mse = 0\n",
        "    for index, value in enumerate(y_true):\n",
        "      pre_mse += (value - y_pred[index])**2\n",
        "    mse = pre_mse/n\n",
        "    return mse\n",
        "\n",
        "  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n",
        "                              weights: list = None, max_steps: int = 10000, \\\n",
        "                              learning_rate: float = 0.003, \\\n",
        "                              save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Градиентный спуск для нескольких переменных\n",
        "\n",
        "    :param X_true: фактические аттрибуты\n",
        "    :param y_true: фактические результаты\n",
        "    :param weights: стартовые веса, если не хотим начать обучение с рандома\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                        если значение отличное от нуля, \n",
        "                        будет сохраняться каждый i-ый шаг\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # Код для данных с 1 аттрибутом\n",
        "    if (type(X_true[0])==int) or (type(X_true[0])==float):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i]=[x,1]\n",
        "    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i].append(1)\n",
        "\n",
        "    # Создаем рандомный спиок весов данных равный количеству атрибутов\n",
        "    if weights == None:\n",
        "      weights = [self.random.random() for f in X_true[0]]\n",
        "\n",
        "    if save_steps > 0:\n",
        "      mse_list = []\n",
        "      weights_list = []\n",
        "    \n",
        "    # MSE предыдущего шага\n",
        "    mse_prev = 0\n",
        "    mse = 999\n",
        "\n",
        "    # Кол-во экспериментов, которые у нас есть      \n",
        "    n = len(X_true)\n",
        "\n",
        "    step = 0\n",
        "    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n",
        "      # Считаем градиенты\n",
        "      gradients = []\n",
        "      for wi, w_value in enumerate(weights):\n",
        "        current_gradient=0\n",
        "        for yi, y_t_val in enumerate(y_true):\n",
        "          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n",
        "                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n",
        "        current_gradient = current_gradient/n\n",
        "        gradients.append(current_gradient)\n",
        "\n",
        "      # Делаем сдвиг весов\n",
        "      for gi, gr_value in enumerate(gradients):\n",
        "        weights[gi] = weights[gi] - learning_rate*gr_value\n",
        "\n",
        "      #Считаем y_pred\n",
        "      y_pred = []\n",
        "      for X_current in X_true:\n",
        "        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n",
        "      \n",
        "      step +=1\n",
        "      mse_prev = mse\n",
        "      mse = self.mse_function(y_true, y_pred)\n",
        "      self.progress_tracker(step, mse)\n",
        "\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          weights_list.append(weights)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n",
        "                      'mse_list': mse_list, 'weights_list': weights_list}\n",
        "    else:\n",
        "      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQSKGR-p8Ge"
      },
      "source": [
        "# Сначала, заполним x. От -10 до 10 с шагом 0.1\n",
        "# И по нему построим y через формулу 5*x + 6\n",
        "\n",
        "# X_true - признаки, которые мы будем подавать в функцию, чтобы получить y_pred\n",
        "X_true = []\n",
        "y_true = []\n",
        "\n",
        "# range работает только с целыми. Умножим все на 10, а при append, поделим на 10\n",
        "for i in range(-100, 100, 1):\n",
        "  X_true.append(i/10)\n",
        "  y_true.append(5*i/10+6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qABdTu1p8jV"
      },
      "source": [
        "new_grad = GradientDescents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttuY0NMbqFhF",
        "outputId": "6c183c3f-7ab3-46c5-c42b-43eea82cd9e5"
      },
      "source": [
        "gr = new_grad.gradient_descent_multi(X_true, y_true)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Шаг: 869\n",
            "Функция потерь: 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMoNYMhrqRM-",
        "outputId": "8ddd7d79-e4ee-4c67-bee1-d3a8ab13db66"
      },
      "source": [
        "gr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mse': 0.0008162586030876157,\n",
              " 'steps': 868,\n",
              " 'weights': [4.999955819944541, 5.971428689818899]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NzUO5huqjL9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}