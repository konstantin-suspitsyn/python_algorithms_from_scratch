{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vID0WUUXxjAZ"
      },
      "source": [
        "# Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THsm-yOixnVq"
      },
      "source": [
        "## Вступление\n",
        "<br>\n",
        "Я разбираюсь в математике оптимизационных алгоритмов без использования готовых математических функций. В предыдущих ноутбуках, уже сформулированы:\n",
        "\n",
        "1. Batch Gradient Descent\n",
        "2. Stochastic Gradient Descent with Momentum\n",
        "\n",
        "Настало время Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGMbYyft0pO5"
      },
      "source": [
        "# Теория"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chd5LA47yWc6"
      },
      "source": [
        "Вновь, возьму статью из архива [An overview of gradient descent optimizationalgorithms](https://arxiv.org/pdf/1609.04747.pdf).<br> Более подробная статья [Adam: A Method  for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf).<br> \n",
        "Много всего было написано в предыдущих ноутбуках. Здесь я помещу только формулы:<br><br>\n",
        "$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$<br>\n",
        "$v_t = \\beta_2v_{t-1} + (1-\\beta_2)g_t^2$<br><br>\n",
        "$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$<br>\n",
        "$\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$<br>\n",
        "$\\theta_{t+1} = \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}*\\hat{m}_t$<br>\n",
        "Где:<br>\n",
        "$g_t$ — градиент<br>\n",
        "$g_t^2$ — квадрат каждого элемента из градиентов<br>\n",
        "$\\alpha$ — learning rate<br>\n",
        "$\\beta_1$ — первый коэффициент обучения. Обычно равен 0.9<br>\n",
        "$\\beta_2$ — первый коэффициент обучения. Обычно равен 0.999<br>\n",
        "$\\epsilon$ обычно $10^{-8}$\n",
        "$t$ — шаг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeLr4Nv7xLP_"
      },
      "source": [
        "class GradientDescents:\n",
        "  \n",
        "  '''\n",
        "  Имплементация градиентного спуска с нуля\n",
        "  '''\n",
        "\n",
        "  import random\n",
        "\n",
        "  def progress_tracker(self, step: int, cost_function: float) -> None:\n",
        "    '''\n",
        "    Функция позволяет отслеживать онлайн прогресс\n",
        "\n",
        "    :param step: текущий шаг\n",
        "    :param cost_function: значение кост-функции в данный момент\n",
        "\n",
        "    '''\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "    print('Шаг: {}'.format(step))\n",
        "    print('Функция потерь: {:.2f}'.format(cost_function))\n",
        "\n",
        "  def mse_function(self, y_true: list, y_pred: list) -> float:\n",
        "    '''\n",
        "    Функция, которая считет MSE\n",
        "\n",
        "    :param y_true: значения y, которые мы знаем из фактических данных\n",
        "    :param y_pred: значения y, которые мы получили в данный момент\n",
        "\n",
        "    :return mse: значеник MSE по формуле\n",
        "    '''\n",
        "    # Кол-во значений, которое мы сравнивам\n",
        "    n = len(y_true)\n",
        "    # Стартуем с нуля\n",
        "    pre_mse = 0\n",
        "    for index, value in enumerate(y_true):\n",
        "      pre_mse += (value - y_pred[index])**2\n",
        "    mse = pre_mse/n\n",
        "    return mse\n",
        "\n",
        "  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n",
        "                              weights: list = None, max_steps: int = 10000, \\\n",
        "                              learning_rate: float = 0.003, \\\n",
        "                              save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Градиентный спуск для нескольких переменных\n",
        "\n",
        "    :param X_true: фактические аттрибуты\n",
        "    :param y_true: фактические результаты\n",
        "    :param weights: стартовые веса, если не хотим начать обучение с рандома\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                        если значение отличное от нуля, \n",
        "                        будет сохраняться каждый i-ый шаг\n",
        "\n",
        "    :return {\n",
        "      :return weights: веса регресси\n",
        "      :return mse: значение MSE\n",
        "      :return steps: количество шагов\n",
        "      :return mse_list: значение MSE во вреия обучения, если save_steps > 0\n",
        "      :return weights_list: веса по мере обучения, если save_steps > 0\n",
        "    }\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # Код для данных с 1 аттрибутом\n",
        "    if (type(X_true[0])==int) or (type(X_true[0])==float):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i]=[x,1]\n",
        "    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i].append(1)\n",
        "\n",
        "    # Создаем рандомный спиок весов данных равный количеству атрибутов\n",
        "    if weights == None:\n",
        "      weights = [self.random.random() for f in X_true[0]]\n",
        "\n",
        "    if save_steps > 0:\n",
        "      mse_list = []\n",
        "      weights_list = []\n",
        "    \n",
        "    # MSE предыдущего шага\n",
        "    mse_prev = 0\n",
        "    mse = 999\n",
        "\n",
        "    # Кол-во экспериментов, которые у нас есть      \n",
        "    n = len(X_true)\n",
        "\n",
        "    step = 0\n",
        "    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n",
        "      # Считаем градиенты\n",
        "      gradients = []\n",
        "      for wi, w_value in enumerate(weights):\n",
        "        current_gradient=0\n",
        "        for yi, y_t_val in enumerate(y_true):\n",
        "          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n",
        "                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n",
        "        current_gradient = current_gradient/n\n",
        "        gradients.append(current_gradient)\n",
        "\n",
        "      # Делаем сдвиг весов\n",
        "      for gi, gr_value in enumerate(gradients):\n",
        "        weights[gi] = weights[gi] - learning_rate*gr_value\n",
        "\n",
        "      #Считаем y_pred\n",
        "      y_pred = []\n",
        "      for X_current in X_true:\n",
        "        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n",
        "      \n",
        "      step +=1\n",
        "      mse_prev = mse\n",
        "      mse = self.mse_function(y_true, y_pred)\n",
        "      self.progress_tracker(step, mse)\n",
        "\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          weights_list.append(weights)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n",
        "                      'mse_list': mse_list, 'weights_list': weights_list}\n",
        "    else:\n",
        "      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict\n",
        "\n",
        "  def momentum_gradient_descent(self, X_true: list, y_true: list, \\\n",
        "                                weights: list = None, max_steps: int = 10000, \\\n",
        "                                learning_rate: float = 0.003, gamma: float = 0.9, \\\n",
        "                                save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Градиентный спуск с ускорением\n",
        "\n",
        "    :param X_true: фактические аттрибуты\n",
        "    :param y_true: фактические результаты\n",
        "    :param weights: стартовые веса, если не хотим начать обучение с рандома\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                        если значение отличное от нуля, \n",
        "                        будет сохраняться каждый i-ый шаг\n",
        "\n",
        "    :return {\n",
        "      :return weights: веса регресси\n",
        "      :return mse: значение MSE\n",
        "      :return steps: количество шагов\n",
        "      :return mse_list: значение MSE во вреия обучения, если save_steps > 0\n",
        "      :return weights_list: веса по мере обучения, если save_steps > 0\n",
        "    }\n",
        "                      \n",
        "    '''\n",
        "    # Код для данных с 1 аттрибутом\n",
        "    if (type(X_true[0])==int) or (type(X_true[0])==float):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i]=[x,1]\n",
        "    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i].append(1)\n",
        "\n",
        "\n",
        "    if save_steps > 0:\n",
        "      mse_list = []\n",
        "      weights_list = []\n",
        "\n",
        "    step = 0\n",
        "    mse_prev = 999999999\n",
        "\n",
        "\n",
        "    # Для чистоты выбора алгоритма, я буду стартовать веса с 1\n",
        "    if weights == None:\n",
        "      weights = [self.random.random() for f in X_true[0]]\n",
        "\n",
        "    # Кол-во элементов в строке X\n",
        "    n = len(X_true[0])\n",
        "\n",
        "    # Стартуем градиенты\n",
        "    # Текущий градиент\n",
        "    gradient = []\n",
        "    # Предыдущий сдвиг\n",
        "    v_t_previous = [0] * len(X_true[0])\n",
        "    \n",
        "    # TO DO: По-хорошему, надо написать условие по тестовой выборке\n",
        "    while step < max_steps:\n",
        "\n",
        "      # Берем номер для выбора данных\n",
        "      index = self.random.randint(0, n-1)\n",
        "      # X и y текущего шага\n",
        "      X_current = X_true[index]\n",
        "      y_current = y_true[index]\n",
        "      gradient = []\n",
        "      # Вычисляем текущий градиента\n",
        "      for x_i in X_current:\n",
        "        current_gradient = -2*(y_current - sum([w*x for w,x in \\\n",
        "                                              zip(weights,X_current)]))*x_i\n",
        "        gradient.append(current_gradient)\n",
        "      \n",
        "      # Применяем ускорение к предыдущему сдвигу\n",
        "      momentum_v_t_previous = [f*gamma for f in v_t_previous]\n",
        "      # Применяем шаг к градиенту\n",
        "      step_gradient = [f*learning_rate for f in gradient]\n",
        "      # Получаем новый сдвиг\n",
        "      v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n",
        "      # В предыдущий сдвиг записываем новый\n",
        "      v_t_previous = v_t\n",
        "\n",
        "      # Делаем сдвиг весов\n",
        "      for vti, vti_value in enumerate(v_t):\n",
        "        weights[vti] = weights[vti] - vti_value\n",
        "\n",
        "      y_pred = sum([w*x for w,x in zip(weights,X_current)])\n",
        "\n",
        "      y_pred_algo = []\n",
        "      for X_current in X_true:\n",
        "        y_pred_algo.append(sum([w*x for w,x in zip(weights,X_current)]))\n",
        "\n",
        "      \n",
        "      mse = self.mse_function(y_pred_algo, y_true)\n",
        "\n",
        "      if mse < mse_prev:\n",
        "        # Сохраняю только лучший результат\n",
        "        # Опять же mse правильнее было бы считать на тестовых данных\n",
        "        final_weights = weights\n",
        "\n",
        "      mse_prev = mse\n",
        "\n",
        "      step += 1\n",
        "\n",
        "      self.progress_tracker(step, mse)\n",
        "\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          weights_list.append(weights)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1, \\\n",
        "                      'mse_list': mse_list, 'weights_list': weights_list}\n",
        "    else:\n",
        "      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict\n",
        "\n",
        "  def adam(self, X_true: list, y_true: list, \\\n",
        "            weights: list = None, max_steps: int = 10000, \\\n",
        "            learning_rate: float = 0.003, \\\n",
        "            beta_1: float = 0.9, beta_2: float = 0.999, eps: int = 1e-8, \\\n",
        "            save_steps: int = 0) -> dict:\n",
        "    '''\n",
        "    Adam\n",
        "\n",
        "    :param X_true: фактические аттрибуты\n",
        "    :param y_true: фактические результаты\n",
        "    :param weights: стартовые веса, если не хотим начать обучение с рандома\n",
        "    :param learning_rate: коэффициент обучения\n",
        "    :param max_steps: максимальное кол-во шагов, при которых алгоритм остановится\n",
        "    :param beta_1:\n",
        "    :param beta_2:\n",
        "    :param eps:\n",
        "    :param save_steps: если 0, сохранится только последний шаг\n",
        "                        если значение отличное от нуля, \n",
        "                        будет сохраняться каждый i-ый шаг\n",
        "\n",
        "    :return {\n",
        "      :return weights: веса регресси\n",
        "      :return mse: значение MSE\n",
        "      :return steps: количество шагов\n",
        "      :return mse_list: значение MSE во вреия обучения, если save_steps > 0\n",
        "      :return weights_list: веса по мере обучения, если save_steps > 0\n",
        "    }\n",
        "    \n",
        "    '''\n",
        "    import math\n",
        "\n",
        "    # Код для данных с 1 аттрибутом\n",
        "    if (type(X_true[0])==int) or (type(X_true[0])==float):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i]=[x,1]\n",
        "    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n",
        "      for i, x in enumerate(X_true):\n",
        "        X_true[i].append(1)\n",
        "\n",
        "\n",
        "    if save_steps > 0:\n",
        "      mse_list = []\n",
        "      weights_list = []\n",
        "\n",
        "    step = 0\n",
        "    mse_prev = 999999999\n",
        "\n",
        "\n",
        "    # Для чистоты выбора алгоритма, я буду стартовать веса с 1\n",
        "    if weights == None:\n",
        "      weights = [self.random.random() for f in X_true[0]]\n",
        "\n",
        "    # Кол-во элементов в строке X\n",
        "    n = len(X_true[0])\n",
        "\n",
        "    m_prev = [0]*n\n",
        "    v_prev = [0]*n\n",
        "\n",
        "    while step < max_steps:\n",
        "\n",
        "      t = step + 1\n",
        "\n",
        "      # Берем номер для выбора данных\n",
        "      index = self.random.randint(0, n-1)\n",
        "      # X и y текущего шага\n",
        "      X_current = X_true[index]\n",
        "      y_current = y_true[index]\n",
        "      gradient = []\n",
        "      # Вычисляем текущий градиента\n",
        "      for x_i in X_current:\n",
        "        current_gradient = -2*(y_current - sum([w*x for w,x in \\\n",
        "                                              zip(weights,X_current)]))*x_i\n",
        "        gradient.append(current_gradient)\n",
        "      \n",
        "      gradient_2 = [f*f for f in gradient]\n",
        "\n",
        "      m_a = [beta_1 * l for l in m_prev]\n",
        "      m_b = [(1 - beta_1) * l for l in gradient]\n",
        "      m = [a+b for a,b in zip(m_a, m_b)]\n",
        "\n",
        "      v_a = [beta_2 * l for l in v_prev]\n",
        "      v_b = [(1 - beta_2) * l for l in gradient_2]\n",
        "      v = [a+b for a,b in zip(v_a, v_b)]\n",
        "\n",
        "      m_hat = [f/(1-beta_1**t) for f in m]\n",
        "      v_hat = [f/(1-beta_2**t) for f in v]\n",
        "\n",
        "      move_vec = [learning_rate * mv /(math.sqrt(vv)+eps) for mv, vv in zip(m, v)]\n",
        "\n",
        "      new_weights = [weigh - mv for mv, weigh in zip(move_vec, weights)]\n",
        "\n",
        "      weights = new_weights\n",
        "\n",
        "      m_prev = m\n",
        "      v_prev = v\n",
        "      \n",
        "      y_pred = sum([w*x for w,x in zip(weights,X_current)])\n",
        "\n",
        "      y_pred_algo = []\n",
        "      for X_current in X_true:\n",
        "        y_pred_algo.append(sum([w*x for w,x in zip(weights,X_current)]))\n",
        "\n",
        "      \n",
        "      mse = self.mse_function(y_pred_algo, y_true)\n",
        "\n",
        "      if mse < mse_prev:\n",
        "        # Сохраняю только лучший результат\n",
        "        # Опять же mse правильнее было бы считать на тестовых данных\n",
        "        final_weights = weights\n",
        "\n",
        "      mse_prev = mse\n",
        "\n",
        "      step += 1\n",
        "\n",
        "      self.progress_tracker(step, mse)\n",
        "\n",
        "      if save_steps > 0:\n",
        "        if step % save_steps == 0:\n",
        "          mse_list.append(mse)\n",
        "          weights_list.append(weights)\n",
        "\n",
        "    if save_steps > 0:\n",
        "      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1, \\\n",
        "                      'mse_list': mse_list, 'weights_list': weights_list}\n",
        "    else:\n",
        "      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1}\n",
        "\n",
        "    return return_dict\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfU-OFL4xlJc"
      },
      "source": [
        "# Загрузим данные из Boston Dataset\n",
        "from sklearn.datasets import load_boston\n",
        "X, y = load_boston(return_X_y=True)\n",
        "X_true = []\n",
        "for i in X:\n",
        "  x_s_list = [f for f in i]\n",
        "  # x_s_list.append(1)\n",
        "  X_true.append(x_s_list)\n",
        "y_true = [f for f in y]\n",
        "del X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGngXPcyLjcJ"
      },
      "source": [
        "msgd = GradientDescents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjfuKB2CL7d7",
        "outputId": "91996631-f977-4aed-9001-cf6049831736"
      },
      "source": [
        "gd = msgd.adam(X_true, y_true, learning_rate=0.003, max_steps=10000, save_steps=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Шаг: 10000\n",
            "Функция потерь: 895.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3bBdvLWL9xB"
      },
      "source": [
        "y_pred_algo_1 = []\n",
        "for X_current in X_true:\n",
        "  y_pred_algo_1.append(sum([w*x for w,x in zip(gd['weights'],X_current)]))\n",
        "\n",
        "mse_algo_1 = msgd.mse_function(y_pred_algo_1, y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_GBktFMqmB",
        "outputId": "c3b919af-9059-4313-ebdc-958de1b2825f"
      },
      "source": [
        "mse_algo_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "895.829626236374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "4f_QLMwGMrBp",
        "outputId": "03399e8c-d097-4dcb-c12f-ebe544d9dd64"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "steps = [i+1 for i,v in enumerate(gd['mse_list'])]\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "ax = fig.add_subplot()\n",
        "ax.plot(steps, gd['mse_list'], color='#ff6361', \\\n",
        "        label='Adam')\n",
        "plt.title('Adam MSE')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYQXFBbdNWwB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}